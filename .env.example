# Inference server config (copy to .env and set as needed).
# Model: Hugging Face model id or local path
INFERENCE_MODEL_ID=./Qwen-14B
# Attention: sdpa (built-in) | flash_attention_2 (install flash-attn for GPU)
INFERENCE_ATTN_IMPLEMENTATION=sdpa
# 4-bit quant compute dtype: float16 | bfloat16
INFERENCE_BNB_COMPUTE_DTYPE=float16
