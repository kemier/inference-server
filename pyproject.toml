[build-system]
requires = ["setuptools>=61.0"]
build-backend = "setuptools.build_meta"

[project]
name = "inference-server-lib"
version = "0.1.0"
description = "LLM Inference Server using ChatOpenAI (for OpenAI-compatible APIs like DeepSeek)"
authors = [{ name = "Your Name", email = "you@example.com" }] # Please update this
requires-python = ">=3.9"
dependencies = [
    "uvicorn[standard]>=0.20.0,<0.30.0",
    "starlette>=0.30.0,<0.40.0",
    "pydantic>=2.0.0,<3.0.0",
    "langchain-core~=0.3.58",          # Back to recent version
    "langchain-openai~=0.3.16",        # Back to recent version
    # "langchain-anthropic~=0.3.12",     # Removed
    # "langchain-deepseek~=0.1.3",       # Removed
    "python-dotenv>=1.0.0,<2.0.0",
    "requests>=2.30.0,<3.0.0",
    "cffi>=1.15.0,<2.0.0",
    "cryptography>=40.0.0,<45.0.0",
    "pillow>=10.0.0,<12.0.0",
    "pycparser>=2.20,<3.0.0"
]

[project.scripts]
# If you want to define a command-line script, e.g.
# run-inference-server = "inference_server_lib.server_starlette:starlette_app"

[tool.setuptools.packages.find]
where = ["src"]  # Look for packages in src
include = ["inference_server_lib*"]  # Include your package
# exclude = []  # Optional: exclude specific sub-packages or modules

[tool.uv]
# UV specific configurations can go here if needed in the future 